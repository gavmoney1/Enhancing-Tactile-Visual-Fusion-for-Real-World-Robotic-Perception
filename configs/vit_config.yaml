# Vision Transformer specific configuration

model:
  patch_size: 16
  num_layers: 12
  num_heads: 12
  projection_dim: 768
  mlp_dim: 3072
  dropout: 0.1

# ViT-specific configuration
embed_dim: 384
mlp_ratio: 4.0

# Classification-specific settings
classification:
  dropout_rate: 0.2
  head_hidden_dim: 512  # Optional hidden layer in classification head